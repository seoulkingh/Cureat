import requests
import json
import os
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from readability import Document
import google.generativeai as genai

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ ì„¤ì •
KAKAO_REST_KEY = os.getenv('KAKAO_REST_KEY')
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')

# Gemini AI ì„¤ì •
genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel('gemini-1.5-flash')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class RestaurantCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        })
        
    def kakao_search_local(self, query: str, size: int = 10) -> List[Dict[str, Any]]:
        """ì¹´ì¹´ì˜¤ ì§€ì—­ ê²€ìƒ‰ APIë¡œ ë§›ì§‘ ê²€ìƒ‰"""
        if not KAKAO_REST_KEY:
            logging.error("ì¹´ì¹´ì˜¤ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
            
        url = "https://dapi.kakao.com/v2/search/local.json"
        headers = {"Authorization": f"KakaoAK {KAKAO_REST_KEY}"}
        params = {
            "query": query,
            "size": size
        }
        
        try:
            response = self.session.get(url, headers=headers, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                documents = data.get('documents', [])
                logging.info(f"ì¹´ì¹´ì˜¤ API: {len(documents)}ê°œ ë§›ì§‘ ì°¾ìŒ")
                return documents
            else:
                logging.error(f"ì¹´ì¹´ì˜¤ API ì˜¤ë¥˜: {response.status_code}")
                return []
        except Exception as e:
            logging.error(f"ì¹´ì¹´ì˜¤ API ìš”ì²­ ì˜¤ë¥˜: {e}")
            return []
    
    def kakao_search_web(self, query: str, size: int = 5) -> List[Dict[str, Any]]:
        """ì¹´ì¹´ì˜¤ ì›¹ ê²€ìƒ‰ APIë¡œ ë¦¬ë·° ê²€ìƒ‰"""
        if not KAKAO_REST_KEY:
            return []
            
        url = "https://dapi.kakao.com/v2/search/web"
        headers = {"Authorization": f"KakaoAK {KAKAO_REST_KEY}"}
        params = {
            "query": f"{query} ë¦¬ë·° ë§›ì§‘",
            "size": size
        }
        
        try:
            response = self.session.get(url, headers=headers, params=params, timeout=10)
            if response.status_code == 200:
                data = response.json()
                documents = data.get('documents', [])
                logging.info(f"ì¹´ì¹´ì˜¤ ì›¹ ê²€ìƒ‰: {len(documents)}ê°œ ë¦¬ë·° í˜ì´ì§€ ì°¾ìŒ")
                return documents
            else:
                logging.error(f"ì¹´ì¹´ì˜¤ ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {response.status_code}")
                return []
        except Exception as e:
            logging.error(f"ì¹´ì¹´ì˜¤ ì›¹ ê²€ìƒ‰ ìš”ì²­ ì˜¤ë¥˜: {e}")
            return []
    
    def fetch_page_content(self, url: str) -> str:
        """ì›¹í˜ì´ì§€ ë‚´ìš© í¬ë¡¤ë§"""
        try:
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                # Readabilityë¡œ ë³¸ë¬¸ ì¶”ì¶œ
                doc = Document(response.text)
                soup = BeautifulSoup(doc.summary(), 'html.parser')
                text = soup.get_text()
                # í…ìŠ¤íŠ¸ ì •ì œ
                lines = [line.strip() for line in text.split('\n') if line.strip()]
                clean_text = '\n'.join(lines)
                return clean_text[:2000]  # 2000ìë¡œ ì œí•œ
            return ""
        except Exception as e:
            logging.warning(f"í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨ {url}: {e}")
            return ""
    
    def extract_reviews_from_content(self, content: str) -> List[str]:
        """í¬ë¡¤ë§í•œ ë‚´ìš©ì—ì„œ ë¦¬ë·° ì¶”ì¶œ"""
        if not content:
            return []
            
        # ë¦¬ë·° ê´€ë ¨ í‚¤ì›Œë“œ
        review_keywords = [
            "ë§›ìˆ", "ì¶”ì²œ", "ë³„ë¡œ", "ìµœê³ ", "ë¶ˆì¹œì ˆ", "ì¹œì ˆ", 
            "ê°€ê²©", "ê°€ì„±ë¹„", "ì¬ë°©ë¬¸", "ì›¨ì´íŒ…", "ë¶„ìœ„ê¸°", 
            "ì„œë¹„ìŠ¤", "ìŒì‹", "ë§›", "í›„ê¸°"
        ]
        
        lines = content.split('\n')
        reviews = []
        
        for line in lines:
            line = line.strip()
            if len(line) > 20 and len(line) < 200:  # ì ë‹¹í•œ ê¸¸ì´
                if any(keyword in line for keyword in review_keywords):
                    reviews.append(line)
                    
        return reviews[:10]  # ìµœëŒ€ 10ê°œ
    
    def analyze_with_gemini(self, restaurant_name: str, reviews: List[str]) -> Dict[str, Any]:
        """Gemini AIë¡œ ë§›ì§‘ ì •ë³´ ìƒì„¸ ë¶„ì„"""
        if not GOOGLE_API_KEY or not reviews:
            return {}
            
        reviews_text = '\n'.join(reviews)
        
        prompt = f"""
ë‹¤ìŒì€ '{restaurant_name}' ë§›ì§‘ì— ëŒ€í•œ ë¦¬ë·°ë“¤ì…ë‹ˆë‹¤:

{reviews_text}

ìœ„ ë¦¬ë·°ë“¤ì„ ë¶„ì„í•´ì„œ ë‹¤ìŒ í˜•ì‹ì˜ ìƒì„¸í•œ JSONìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

{{
    "restaurant_summary": {{
        "name": "{restaurant_name}",
        "category": "ìŒì‹ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: í•œì‹, ì¼ì‹, ì¤‘ì‹, ì–‘ì‹, ì¹´í˜ ë“±)",
        "overall_rating": 4.2,
        "price_range": "ê°€ê²©ëŒ€ (ì˜ˆ: 1ë§Œì› ì´í•˜, 1-2ë§Œì›, 2-3ë§Œì›, 3ë§Œì› ì´ìƒ)",
        "recommended_for": ["ë°ì´íŠ¸", "ê°€ì¡±ì‹ì‚¬", "íšŒì‹", "í˜¼ë°¥", "ì¹œêµ¬ëª¨ì„"] ì¤‘ ì ì ˆí•œ ê²ƒë“¤
    }},
    "menu_info": {{
        "signature_dishes": ["ëŒ€í‘œë©”ë‰´1", "ëŒ€í‘œë©”ë‰´2", "ëŒ€í‘œë©”ë‰´3"],
        "popular_items": ["ì¸ê¸°ë©”ë‰´1", "ì¸ê¸°ë©”ë‰´2"],
        "menu_variety": "ë©”ë‰´ ë‹¤ì–‘ì„± í‰ê°€",
        "taste_rating": 4.1
    }},
    "ambiance_service": {{
        "atmosphere": "ë¶„ìœ„ê¸° ìƒì„¸ ì„¤ëª…",
        "interior": "ì¸í…Œë¦¬ì–´ íŠ¹ì§•",
        "service_quality": "ì„œë¹„ìŠ¤ í’ˆì§ˆ í‰ê°€",
        "staff_friendliness": 4.0,
        "cleanliness": "ì²­ê²°ë„ í‰ê°€"
    }},
    "practical_info": {{
        "parking": "ì£¼ì°¨ ì •ë³´",
        "waiting_time": "ëŒ€ê¸°ì‹œê°„ ì •ë³´",
        "reservation": "ì˜ˆì•½ ê°€ëŠ¥ ì—¬ë¶€",
        "opening_hours": "ì˜ì—…ì‹œê°„ ì •ë³´ (ìˆë‹¤ë©´)",
        "best_time_to_visit": "ë°©ë¬¸ ì¶”ì²œ ì‹œê°„ëŒ€"
    }},
    "detailed_analysis": {{
        "pros": ["êµ¬ì²´ì ì¸ ì¥ì 1", "êµ¬ì²´ì ì¸ ì¥ì 2", "êµ¬ì²´ì ì¸ ì¥ì 3"],
        "cons": ["êµ¬ì²´ì ì¸ ë‹¨ì 1", "êµ¬ì²´ì ì¸ ë‹¨ì 2", "êµ¬ì²´ì ì¸ ë‹¨ì 3"],
        "keywords": ["íŠ¹ì§•í‚¤ì›Œë“œ1", "íŠ¹ì§•í‚¤ì›Œë“œ2", "íŠ¹ì§•í‚¤ì›Œë“œ3", "íŠ¹ì§•í‚¤ì›Œë“œ4", "íŠ¹ì§•í‚¤ì›Œë“œ5"],
        "customer_types": "ì£¼ìš” ê³ ê°ì¸µ ë¶„ì„",
        "revisit_intention": "ì¬ë°©ë¬¸ ì˜í–¥ ë¶„ì„"
    }},
    "recommendation": {{
        "overall_recommendation": "ì „ì²´ì ì¸ ì¶”ì²œë„ (5ì  ë§Œì )",
        "target_audience": "ì¶”ì²œ ëŒ€ìƒ",
        "best_menu_combo": "ì¶”ì²œ ë©”ë‰´ ì¡°í•©",
        "visit_tips": "ë°©ë¬¸ íŒ"
    }}
}}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        try:
            response = model.generate_content(prompt)
            response_text = response.text.strip()
            
            # JSON ì¶”ì¶œ
            start = response_text.find('{')
            end = response_text.rfind('}') + 1
            if start != -1 and end != 0:
                json_str = response_text[start:end]
                analysis = json.loads(json_str)
                logging.info(f"Gemini ë¶„ì„ ì™„ë£Œ: {restaurant_name}")
                return analysis
            else:
                logging.warning("Gemini ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return {}
                
        except Exception as e:
            logging.error(f"Gemini AI ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {}
    
    def crawl_restaurant(self, restaurant_name: str, location: str = "") -> Dict[str, Any]:
        """ê°œë³„ ë§›ì§‘ í¬ë¡¤ë§ ë° ë¶„ì„"""
        logging.info(f"ë§›ì§‘ í¬ë¡¤ë§ ì‹œì‘: {restaurant_name}")
        
        # 1. ì¹´ì¹´ì˜¤ ì§€ì—­ ê²€ìƒ‰ìœ¼ë¡œ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        search_query = f"{location} {restaurant_name}" if location else restaurant_name
        kakao_results = self.kakao_search_local(search_query, size=5)
        
        restaurant_info = {
            "name": restaurant_name,
            "search_query": search_query,
            "crawled_at": datetime.now().isoformat(),
            "kakao_basic_info": kakao_results[0] if kakao_results else {},
            "reviews": [],
            "analysis": {}
        }
        
        # 2. ì¹´ì¹´ì˜¤ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë¦¬ë·° í˜ì´ì§€ ì°¾ê¸°
        web_results = self.kakao_search_web(search_query, size=10)
        
        all_reviews = []
        
        # 3. ê° ì›¹í˜ì´ì§€ì—ì„œ ë¦¬ë·° í¬ë¡¤ë§
        for web_result in web_results[:5]:  # ìƒìœ„ 5ê°œ í˜ì´ì§€ë§Œ
            url = web_result.get('url', '')
            title = web_result.get('title', '')
            
            # ë” ë„“ì€ ë²”ìœ„ì˜ ì‚¬ì´íŠ¸ì—ì„œ í¬ë¡¤ë§
            if any(site in url for site in ['blog.naver.com', 'tistory.com', 'kakao.com', 'daum.net', 'zum.com']):
                logging.info(f"í¬ë¡¤ë§ ì‹œë„: {url}")
                content = self.fetch_page_content(url)
                if content:
                    page_reviews = self.extract_reviews_from_content(content)
                    logging.info(f"í˜ì´ì§€ì—ì„œ {len(page_reviews)}ê°œ ë¦¬ë·° ì¶”ì¶œ")
                    for review in page_reviews:
                        all_reviews.append({
                            "text": review,
                            "source_url": url,
                            "source_title": title
                        })
        
        restaurant_info["reviews"] = all_reviews[:20]  # ìµœëŒ€ 20ê°œ ë¦¬ë·°
        
        # 4. Gemini AIë¡œ ë¶„ì„
        if all_reviews:
            review_texts = [r["text"] for r in all_reviews]
            analysis = self.analyze_with_gemini(restaurant_name, review_texts)
            restaurant_info["analysis"] = analysis
        
        logging.info(f"ë§›ì§‘ í¬ë¡¤ë§ ì™„ë£Œ: {restaurant_name} ({len(all_reviews)}ê°œ ë¦¬ë·°)")
        return restaurant_info
    
    def crawl_multiple_restaurants(self, restaurant_list: List[str], location: str = "ê°•ë‚¨") -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ ë§›ì§‘ í¬ë¡¤ë§"""
        results = []
        
        for restaurant in restaurant_list:
            try:
                result = self.crawl_restaurant(restaurant, location)
                results.append(result)
            except Exception as e:
                logging.error(f"ë§›ì§‘ í¬ë¡¤ë§ ì˜¤ë¥˜ {restaurant}: {e}")
                continue
                
        return results
    
    def save_to_json(self, data: List[Dict[str, Any]], filename: str = None):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"restaurant_crawling_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logging.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename}")
            return filename
        except Exception as e:
            logging.error(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
            return None


    def print_restaurant_summary(self, results: List[Dict[str, Any]]):
        """ê°€ê²Œë³„ ìš”ì•½ ì •ë³´ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸª ë§›ì§‘ í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        for i, result in enumerate(results, 1):
            name = result.get('name', 'ì•Œ ìˆ˜ ì—†ìŒ')
            review_count = len(result.get('reviews', []))
            analysis = result.get('analysis', {})
            
            print(f"\nğŸ“ {i}. {name}")
            print("-" * 50)
            
            if analysis:
                restaurant_summary = analysis.get('restaurant_summary', {})
                menu_info = analysis.get('menu_info', {})
                detailed_analysis = analysis.get('detailed_analysis', {})
                recommendation = analysis.get('recommendation', {})
                
                # ê¸°ë³¸ ì •ë³´
                category = restaurant_summary.get('category', 'ì •ë³´ ì—†ìŒ')
                rating = restaurant_summary.get('overall_rating', 'ì •ë³´ ì—†ìŒ')
                price_range = restaurant_summary.get('price_range', 'ì •ë³´ ì—†ìŒ')
                recommended_for = restaurant_summary.get('recommended_for', [])
                
                print(f"ğŸ´ ì¹´í…Œê³ ë¦¬: {category}")
                print(f"â­ í‰ì : {rating}")
                print(f"ğŸ’° ê°€ê²©ëŒ€: {price_range}")
                print(f"ğŸ‘¥ ì¶”ì²œ ëŒ€ìƒ: {', '.join(recommended_for) if recommended_for else 'ì •ë³´ ì—†ìŒ'}")
                
                # ë©”ë‰´ ì •ë³´
                signature_dishes = menu_info.get('signature_dishes', [])
                if signature_dishes:
                    print(f"ğŸ¥˜ ëŒ€í‘œë©”ë‰´: {', '.join(signature_dishes)}")
                
                # ì¥ë‹¨ì 
                pros = detailed_analysis.get('pros', [])
                cons = detailed_analysis.get('cons', [])
                
                if pros:
                    print(f"âœ… ì¥ì : {', '.join(pros[:2])}")  # ìƒìœ„ 2ê°œë§Œ
                if cons:
                    print(f"âŒ ë‹¨ì : {', '.join(cons[:2])}")  # ìƒìœ„ 2ê°œë§Œ
                
                # ì¶”ì²œë„
                overall_recommendation = recommendation.get('overall_recommendation', 'ì •ë³´ ì—†ìŒ')
                target_audience = recommendation.get('target_audience', 'ì •ë³´ ì—†ìŒ')
                
                print(f"ğŸ¯ ì¶”ì²œë„: {overall_recommendation}")
                print(f"ğŸ‘¤ ì¶”ì²œ ëŒ€ìƒ: {target_audience}")
                
            else:
                print("ğŸ“ ë¦¬ë·° ì •ë³´ ì—†ìŒ - ë¶„ì„ ë¶ˆê°€")
            
            print(f"ğŸ“Š ìˆ˜ì§‘ëœ ë¦¬ë·°: {review_count}ê°œ")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ì„¤ì • ê°€ëŠ¥í•œ í¬ë¡¤ë§"""
    crawler = RestaurantCrawler()
    
    print("ğŸ” ë§›ì§‘ í¬ë¡¤ë§ í”„ë¡œê·¸ë¨")
    print("=" * 50)
    
    # í¬ë¡¤ë§ ì„¤ì • ì…ë ¥
    try:
        restaurant_count = int(input("í¬ë¡¤ë§í•  ë§›ì§‘ ê°œìˆ˜ (ê¸°ë³¸ 3ê°œ): ") or "3")
        location = input("ì§€ì—­ (ê¸°ë³¸ ê°•ë‚¨): ") or "ê°•ë‚¨"
        
        print("\në§›ì§‘ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš” (Enterë¡œ ì™„ë£Œ):")
        restaurants = []
        
        for i in range(restaurant_count):
            restaurant = input(f"{i+1}. ").strip()
            if restaurant:
                restaurants.append(restaurant)
            elif i == 0:  # ì²« ë²ˆì§¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                restaurants = ["ê°•ë‚¨ì—­ ë§›ì§‘", "ì‚¬ë‹¹ì—­ ë§›ì§‘", "í™ëŒ€ íŒŒìŠ¤íƒ€"]
                break
        
        if not restaurants:
            restaurants = ["ê°•ë‚¨ì—­ ë§›ì§‘", "ì‚¬ë‹¹ì—­ ë§›ì§‘", "í™ëŒ€ íŒŒìŠ¤íƒ€"]
            
    except (ValueError, KeyboardInterrupt):
        # ê¸°ë³¸ê°’ ì‚¬ìš©
        restaurant_count = 3
        location = "ê°•ë‚¨"
        restaurants = ["ê°•ë‚¨ì—­ ë§›ì§‘", "ì‚¬ë‹¹ì—­ ë§›ì§‘", "í™ëŒ€ íŒŒìŠ¤íƒ€"]
    
    print(f"\nğŸš€ í¬ë¡¤ë§ ì‹œì‘: {len(restaurants)}ê°œ ë§›ì§‘ ({location} ì§€ì—­)")
    print("ë§›ì§‘ ëª©ë¡:", ", ".join(restaurants))
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    results = crawler.crawl_multiple_restaurants(restaurants, location)
    
    # ê°€ê²Œë³„ ìš”ì•½ ì •ë³´ ì¶œë ¥
    crawler.print_restaurant_summary(results)
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    filename = crawler.save_to_json(results)
    
    if filename:
        print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {filename}")
        print(f"ì´ {len(results)}ê°œ ë§›ì§‘ ì •ë³´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨")


if __name__ == "__main__":
    main()
